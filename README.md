# ETL Pipeline - Perusahaan XYZ

End-to-end ETL (Extract, Transform, Load) data pipeline menggunakan **Luigi** untuk Perusahaan XYZ. Pipeline ini mengumpulkan data dari 3 sumber berbeda, membersihkannya, dan memuatnya ke Data Warehouse PostgreSQL.

---

## ğŸ“Š Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL DB   â”‚  CSV File         â”‚  Web Scraping             â”‚
â”‚  Amazon Sales    â”‚  Electronics      â”‚  Detik.com Articles       â”‚
â”‚  (Tim Sales)     â”‚  Products         â”‚  (Tim Data Scientist)     â”‚
â”‚                  â”‚  (Tim Product)    â”‚                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                   â”‚                        â”‚
         â–¼                   â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      EXTRACT (Luigi Tasks)                       â”‚
â”‚  ExtractSalesData    ExtractProductsData    ExtractDetikArticles â”‚
â”‚  [INCREMENTAL]       [STATIC]               [STATIC]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚                      â”‚
         â–¼                     â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     TRANSFORM (Luigi Tasks)                      â”‚
â”‚  TransformSalesData  TransformProductsData  TransformReviewsData â”‚
â”‚  [INCREMENTAL]       [STATIC]               [STATIC]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                     â”‚                      â”‚
         â–¼                     â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LOAD to Data Warehouse (UPSERT)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ fact_sales â”‚  â”‚ dim_products â”‚  â”‚   nlp_training_data      â”‚ â”‚
â”‚  â”‚ dim_date   â”‚  â”‚              â”‚  â”‚                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                    PostgreSQL (Docker)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Project Structure

```
ETL Optional Project/
â”œâ”€â”€ main.py                  # Entry point - menjalankan seluruh ETL pipeline
â”œâ”€â”€ Task/
â”‚   â”œâ”€â”€ extract.py           # Extract: PostgreSQL, CSV, Web Scraping
â”‚   â”œâ”€â”€ transform.py         # Transform: Cleansing & validasi data
â”‚   â””â”€â”€ load.py              # Load: UPSERT ke Data Warehouse
â”œâ”€â”€ Scripts/
â”‚   â”œâ”€â”€ run_etl.sh           # Shell script runner (Linux/WSL)
â”‚   â”œâ”€â”€ setup_crontab.sh     # Setup cron scheduling
â”‚   â”œâ”€â”€ monitor.sh           # Monitoring pipeline
â”‚   â””â”€â”€ cleanup.sh           # Cleanup logs & temp files
â”œâ”€â”€ Data Source/
â”‚   â””â”€â”€ ElectronicsProductsPricingData.csv  # Source data produk (static)
â”œâ”€â”€ docker-compose.yml       # Docker setup untuk Data Warehouse
â”œâ”€â”€ dockerfile               # PostgreSQL 15 Alpine image
â”œâ”€â”€ init.sql                 # SQL inisialisasi DW
â”œâ”€â”€ requirement.txt          # Python dependencies
â”œâ”€â”€ run_etl.bat              # Windows batch runner
â”œâ”€â”€ setup_scheduler.ps1      # Windows Task Scheduler setup
â”œâ”€â”€ ETL_Pipeline_Design.md   # Dokumentasi design pipeline
â”œâ”€â”€ sales data.ipynb         # Jupyter Notebook analisis
â”œâ”€â”€ study case.txt           # Deskripsi study case
â”œâ”€â”€ .env.example             # Template environment variables
â”œâ”€â”€ .gitignore               # Git ignore rules
â””â”€â”€ README.md                # Dokumentasi ini
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python** 3.10+
- **Docker** & Docker Compose
- **PostgreSQL** (source database untuk sales data)
- **WSL/Linux** (untuk cron scheduling, opsional)

### 1. Clone Repository

```bash
git clone <repository-url>
cd "ETL Optional Project"
```

### 2. Setup Virtual Environment

```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/WSL
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirement.txt
```

### 4. Setup Environment Variables

```bash
cp .env.example .env
```

Edit `.env` dan isi kredensial database:

```dotenv
DATABASE_URL=postgresql://postgres:your_password@localhost:5432/etl_db
WAREHOUSE_URL=postgresql://dw_user:dw_password@localhost:5433/xyz_warehouse
LOG_LEVEL=INFO
```

### 5. Start Data Warehouse (Docker)

```bash
docker-compose up -d
```

Verifikasi container berjalan:

```bash
docker ps
# xyz_data_warehouse should be running on port 5433
```

### 6. Pastikan Source Database Tersedia

Source database PostgreSQL (port `5432`) harus sudah berisi tabel `amazon_sales_data`.

---

## â–¶ï¸ Menjalankan Pipeline

### Full Pipeline

```bash
python main.py
```

### Per Phase

```bash
python main.py extract      # Hanya extract
python main.py transform    # Hanya transform
python main.py load         # Hanya load
```

### Utilitas

```bash
python main.py clean        # Hapus semua output files (force re-run)
python main.py help         # Tampilkan bantuan
```

### Via Shell Script (Linux/WSL)

```bash
chmod +x Scripts/run_etl.sh
./Scripts/run_etl.sh
```

### Via Batch File (Windows)

```cmd
run_etl.bat
```

---

## ğŸ“… Scheduling (Cron)

### Setup Cron Job (Linux/WSL)

```bash
chmod +x Scripts/setup_crontab.sh
./Scripts/setup_crontab.sh
```

Default schedule: **setiap 5 menit** (dapat diubah di `setup_crontab.sh`).

### Untuk Production (Daily)

Edit `Scripts/setup_crontab.sh`, uncomment baris:

```bash
# Daily at 02:00 AM
0 2 * * * /path/to/run_etl.sh >> /path/to/logs/cron_$(date +%Y%m%d).log 2>&1
```

### Windows Task Scheduler

```powershell
.\setup_scheduler.ps1
```

---

## ğŸ”„ Strategy: Incremental vs Static

Pipeline menggunakan dua strategi berbeda berdasarkan sifat data:

| Data Source | Strategy | Behavior |
|---|---|---|
| **Sales Data** | INCREMENTAL | `complete()` menggunakan `_is_file_fresh()` â†’ selalu rerun, UPSERT ke DW |
| **Products Data** | STATIC | `complete()` cek flag file â†’ run sekali, skip pada run berikutnya |
| **NLP Data** | STATIC | `complete()` cek flag file â†’ run sekali, skip pada run berikutnya |
| **dim_date** | STATIC | `complete()` cek DB count â†’ run sekali |

### Bagaimana INCREMENTAL Bekerja

```
Pipeline Run #1 (t=0)
  flag tidak ada / file lama â†’ complete()=False â†’ run task
  â†’ UPSERT ke DW â†’ flag ditulis â†’ complete()=True âœ…

Pipeline Run #2 (t < 5 menit)
  flag masih fresh (< 300s) â†’ complete()=True â†’ SKIP â­ï¸

Pipeline Run #3 (t > 5 menit)
  flag sudah tua (> 300s) â†’ complete()=False â†’ rerun task âœ…
```

---

## ğŸ—ï¸ Data Warehouse Schema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_date   â”‚     â”‚   dim_products   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ date_id (PK) â”‚     â”‚ product_id (PK)  â”‚
â”‚ full_date    â”‚     â”‚ product_name     â”‚
â”‚ year         â”‚     â”‚ brand            â”‚
â”‚ month        â”‚     â”‚ manufacturer     â”‚
â”‚ day          â”‚     â”‚ main_category    â”‚
â”‚ quarter      â”‚     â”‚ prices_min       â”‚
â”‚ day_of_week  â”‚     â”‚ prices_max       â”‚
â”‚ day_name     â”‚     â”‚ prices_average   â”‚
â”‚ month_name   â”‚     â”‚ prices_currency  â”‚
â”‚ is_weekend   â”‚     â”‚ availability     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ FK
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    fact_sales       â”‚     â”‚  nlp_training_data    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sale_id (PK)        â”‚     â”‚ text_id (PK)          â”‚
â”‚ product_name        â”‚     â”‚ article_id            â”‚
â”‚ main_category       â”‚     â”‚ judul                 â”‚
â”‚ sub_category        â”‚     â”‚ deskripsi             â”‚
â”‚ date_id (FK)        â”‚     â”‚ category              â”‚
â”‚ discount_price      â”‚     â”‚ url (UNIQUE)          â”‚
â”‚ actual_price        â”‚     â”‚ tanggal_artikel       â”‚
â”‚ discount_percentage â”‚     â”‚ scraped_date          â”‚
â”‚ ratings             â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ no_of_ratings       â”‚
â”‚ image_url           â”‚
â”‚ product_link        â”‚
â”‚ UNIQUE(product_name,â”‚
â”‚   product_link)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Catatan:** `fact_sales` dan `dim_products` **tidak memiliki relasi FK** karena domain berbeda:
- Sales data berisi **semua kategori** produk Amazon (fashion, electronics, dll)
- Products data berisi **khusus produk elektronik** saja

---

## ğŸ§ª Testing Scenario

### 1. Insert Data Baru di Source

```sql
-- Connect ke source DB (port 5432)
INSERT INTO amazon_sales_data 
  ("name", main_category, sub_category, image, link, ratings, no_of_ratings, discount_price, actual_price)
VALUES 
  ('Testing Product', 'Testing Category', 'Testing Sub Category', 
   'https://example.com/image.png', 'https://example.com/', 5, 30, 450, 1000);
```

### 2. Jalankan Pipeline

```bash
python main.py
```

### 3. Verifikasi di Data Warehouse

```sql
-- Connect ke DW (port 5433)
SELECT * FROM fact_sales 
WHERE product_name = 'Testing Product';
```

Data testing berhasil jika record **muncul di Data Warehouse** setelah pipeline dijalankan.

---

## ğŸ“ Logs

Pipeline log disimpan di folder `logs/`:

```
logs/
â”œâ”€â”€ etl_pipeline_YYYYMMDD_HHMMSS.log   # Pipeline execution log
â”œâ”€â”€ warehouse_tables_created.flag        # DW tables creation flag
â”œâ”€â”€ dim_date_loaded.flag                 # Date dimension flag
â”œâ”€â”€ dim_products_loaded.flag             # Products dimension flag
â”œâ”€â”€ fact_sales_loaded.flag               # Sales fact flag (INCREMENTAL)
â”œâ”€â”€ nlp_training_data_loaded.flag        # NLP data flag
â””â”€â”€ flags/                               # Task-level flags
    â”œâ”€â”€ .extract_products.done
    â”œâ”€â”€ .extract_detik.done
    â”œâ”€â”€ .transform_products.done
    â””â”€â”€ .transform_reviews.done
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|---|---|
| **Orchestrator** | Luigi |
| **Language** | Python 3.10+ |
| **Source DB** | PostgreSQL 15 |
| **Data Warehouse** | PostgreSQL 15 (Docker) |
| **Containerization** | Docker & Docker Compose |
| **Scheduling** | Crontab (Linux/WSL) / Task Scheduler (Windows) |
| **Web Scraping** | BeautifulSoup4 + Requests |
| **Data Processing** | Pandas |
| **ORM** | SQLAlchemy |

---

## ğŸ‘¤ Author

Data Engineer Team - Perusahaan XYZ

---

## ğŸ“„ License

This project is for educational purposes (Pacmann - Intro to Data Engineering).
