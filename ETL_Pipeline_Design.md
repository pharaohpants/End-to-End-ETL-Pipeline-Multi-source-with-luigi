# ETL Pipeline Design - Amazon Sales & Electronics Data Warehouse

## üìä Overview Pipeline Architecture

```mermaid
graph TB
    subgraph EXTRACT["üîç EXTRACT PHASE"]
        E1[PostgreSQL<br/>Amazon Sales Data]
        E2[CSV File<br/>Electronics Products]
        E3[Web Scraping<br/>Detik.com Articles]
    end
    
    subgraph TRANSFORM["‚öôÔ∏è TRANSFORM PHASE"]
        T1[Sales Data Cleaning]
        T2[Products Data Cleaning]
        T3[NLP Data Cleaning]
    end
    
    subgraph LOAD["üì• LOAD PHASE"]
        L1[Create DW Schema]
        L2[Load dim_date]
        L3[Load dim_products]
        L4[Load fact_sales]
        L5[Load nlp_training_data]
    end
    
    E1 --> T1
    E2 --> T2
    E3 --> T3
    
    T1 --> L4
    T2 --> L3
    T3 --> L5
    
    L1 --> L2
    L1 --> L3
    L1 --> L4
    L1 --> L5
    
    L2 --> L4
```

---

## üîç PHASE 1: EXTRACT

### 1.1 Extract Sales Data (PostgreSQL)

**Source:** PostgreSQL Database ‚Üí `amazon_sales_data` table

**Process Flow:**
```mermaid
graph LR
    A[Connect to PostgreSQL] --> B[Execute Query]
    B --> C[Read Data with Pandas]
    C --> D{Check Data Retrieved?}
    D -->|Yes| E[Save to raw_SalesData.csv]
    D -->|No| F[Raise Error]
    E --> G[Create Session Flag]
    G --> H[Log Success]
```

**Validation Checks:**
- ‚úÖ Database connection successful
- ‚úÖ Query execution without errors
- ‚úÖ Data rows > 0
- ‚úÖ File saved successfully
- ‚úÖ Session flag created

**Output:** `Data Source/raw_SalesData.csv`

---

### 1.2 Extract Products Data (CSV)

**Source:** CSV File ‚Üí `ElectronicsProductsPricingData.csv`

**Process Flow:**
```mermaid
graph LR
    A[Check File Exists] --> B{File Found?}
    B -->|No| C[Raise FileNotFoundError]
    B -->|Yes| D[Read CSV with Pandas]
    D --> E[Validate Columns]
    E --> F[Save to products_raw.csv]
    F --> G[Create Session Flag]
    G --> H[Log Success]
```

**Validation Checks:**
- ‚úÖ Source file exists
- ‚úÖ File readable (valid CSV format)
- ‚úÖ Columns present
- ‚úÖ Data shape logged
- ‚úÖ Session flag created

**Output:** `Data Source/raw/products_raw.csv`

---

### 1.3 Extract Detik Articles (Web Scraping)

**Source:** Website ‚Üí `https://www.detik.com/tag/ai/`

**Process Flow:**
```mermaid
graph TB
    A[Start Scraping Page 1] --> B[Send HTTP Request]
    B --> C{Response OK?}
    C -->|Timeout| D[Retry 3x with Backoff]
    C -->|Success| E[Parse HTML with BeautifulSoup]
    E --> F[Extract Articles]
    F --> G[Validate Required Fields]
    G --> H{Has Next Button?}
    H -->|Yes| I[Go to Next Page]
    H -->|No| J[Concatenate All Articles]
    I --> B
    J --> K[Add article_id]
    K --> L[Save to reviews_raw.csv]
    L --> M[Create Session Flag]
```

**Validation Checks:**
- ‚úÖ HTTP response status 200
- ‚úÖ BeautifulSoup parsing successful
- ‚úÖ Articles found on page
- ‚úÖ Required fields (judul, url) not empty
- ‚úÖ Retry mechanism on timeout (3x with 5s/10s/15s backoff)
- ‚úÖ Stop on 3 consecutive timeouts
- ‚úÖ Session flag created

**Output:** `Data Source/raw/reviews_raw.csv`

---

## ‚öôÔ∏è PHASE 2: TRANSFORM

### 2.1 Transform Sales Data

**Input:** `raw_SalesData.csv`  
**Output:** `Data Source/cleaned/sales_cleaned.csv`

**Process Flow:**
```mermaid
graph TB
    A[Load Raw Data] --> B[Step 1: Drop Unnamed Columns]
    B --> C[Step 2: Clean Product Name]
    C --> D[Step 3: Clean Categories]
    D --> E[Step 4: Clean Ratings]
    E --> F[Step 5: Clean No of Ratings]
    F --> G[Step 6: Clean Prices]
    G --> H[Step 7: Clean URLs]
    H --> I[Step 8: Remove Duplicates]
    I --> J[Step 9: Remove Critical Missing]
    J --> K[Save Cleaned Data]
```

**Detailed Validation & Cleaning Steps:**

| Step | Field | Actions | Validation |
|------|-------|---------|------------|
| 1 | Unnamed Columns | Drop all `Unnamed:*` columns | ‚úÖ Columns cleaned |
| 2 | `name` | Fill NA with 'Unknown Product'<br/>Strip whitespace<br/>Replace empty with 'Unknown Product' | ‚úÖ No nulls<br/>‚úÖ No empty strings |
| 3 | `main_category`<br/>`sub_category` | Fill NA with 'Uncategorized'/'General' | ‚úÖ No nulls |
| 4 | `ratings` | Convert to numeric<br/>Fill NA with 0.0<br/>Cap between 0-5 | ‚úÖ Numeric type<br/>‚úÖ Range [0, 5] |
| 5 | `no_of_ratings` | Convert to numeric<br/>Fill NA with 0<br/>Convert to integer | ‚úÖ Integer type<br/>‚úÖ No nulls |
| 6 | `discount_price`<br/>`actual_price` | Remove ‚Çπ symbol and commas<br/>Convert to float<br/>Fill missing with counterpart<br/>Calculate discount % | ‚úÖ Numeric type<br/>‚úÖ Cross-validation<br/>‚úÖ Discount % calculated |
| 7 | `image`<br/>`link` | Fill NA with empty string | ‚úÖ No nulls |
| 8 | Duplicates | Remove duplicates on (`name`, `link`) | ‚úÖ Count logged<br/>‚úÖ Keep='first' |
| 9 | Critical Missing | Drop rows where `name` is null | ‚úÖ No critical nulls |

**Data Quality Checks:**
- ‚úÖ **Missing Values:** All handled with appropriate defaults
- ‚úÖ **Data Types:** Correct types enforced
- ‚úÖ **Duplicates:** Removed based on business key
- ‚úÖ **Range Validation:** Ratings 0-5, prices non-negative
- ‚úÖ **Referential Integrity:** Discount ‚â§ Actual price

**Output Statistics:**
- Final shape reported
- Unique categories count
- Average rating
- Average price

---

### 2.2 Transform Products Data

**Input:** `raw/products_raw.csv`  
**Output:** `Data Source/cleaned/products_cleaned.csv`

**Process Flow:**
```mermaid
graph TB
    A[Load Raw Data] --> B[Step 1: Drop Unnamed Columns]
    B --> C[Step 2: Clean Product Name]
    C --> D[Step 3: Clean Brand & Manufacturer]
    D --> E[Step 4: Clean Prices Min/Max]
    E --> F[Step 5: Clean Currency]
    F --> G[Step 6: Clean Availability]
    G --> H[Step 7: Clean Condition]
    H --> I[Step 8: Clean isSale]
    I --> J[Step 9: Clean Categories]
    J --> K[Step 10: Remove Duplicates]
    K --> L[Step 11: Remove Empty Rows]
    L --> M[Save Cleaned Data]
```

**Detailed Validation & Cleaning Steps:**

| Step | Field | Actions | Validation |
|------|-------|---------|------------|
| 1 | Unnamed Columns | Drop all `Unnamed:*` columns | ‚úÖ Columns cleaned |
| 2 | `name` | Fill NA with 'Unknown Product'<br/>Strip whitespace | ‚úÖ No nulls<br/>‚úÖ Trimmed |
| 3 | `brand`<br/>`manufacturer` | Fill NA with 'Unknown'<br/>Manufacturer defaults to brand | ‚úÖ No nulls<br/>‚úÖ Logical defaults |
| 4 | `prices.amountMin`<br/>`prices.amountMax` | Convert to numeric<br/>Fill missing with counterpart<br/>Fill remaining with 0<br/>Ensure Max ‚â• Min<br/>Calculate average | ‚úÖ Numeric type<br/>‚úÖ Range validated<br/>‚úÖ Average calculated |
| 5 | `prices.currency` | Fill NA with 'USD'<br/>Uppercase<br/>Strip whitespace | ‚úÖ Standardized format |
| 6 | `prices.availability` | Fill NA with 'Unknown' | ‚úÖ No nulls |
| 7 | `prices.condition` | Fill NA with 'New'<br/>Title case | ‚úÖ Standardized format |
| 8 | `prices.isSale` | Fill NA with False | ‚úÖ Boolean type |
| 9 | `categories`<br/>`primaryCategories` | Fill NA with 'Uncategorized'<br/>Primary defaults to categories | ‚úÖ No nulls |
| 10 | Duplicates | Remove duplicates on (`name`, `brand`) | ‚úÖ Count logged |
| 11 | Empty Rows | Drop completely empty rows | ‚úÖ No all-null rows |

**Data Quality Checks:**
- ‚úÖ **Missing Values:** All handled
- ‚úÖ **Data Types:** Enforced (numeric, boolean, string)
- ‚úÖ **Duplicates:** Removed by business key
- ‚úÖ **Range Validation:** Max ‚â• Min prices
- ‚úÖ **Standardization:** Currency uppercase, condition title case

**Output Statistics:**
- Final shape
- Unique brands count
- Average price

---

### 2.3 Transform Reviews Data (NLP Training)

**Input:** `raw/reviews_raw.csv`  
**Output:** `Data Source/cleaned/reviews_cleaned.csv`

**Process Flow:**
```mermaid
graph TB
    A[Load Raw Data] --> B[Step 1: Remove Empty Rows]
    B --> C[Step 2: Clean Judul]
    C --> D[Step 3: Clean Deskripsi]
    D --> E[Step 4: Clean Tanggal Artikel]
    E --> F[Step 5: Parse Scraped Date]
    F --> G[Step 6: Clean URL]
    G --> H[Step 7: Standardize Category]
    H --> I[Step 8: Remove Duplicates]
    I --> J[Step 9: Filter Invalid Records]
    J --> K[Reset article_id]
    K --> L[Save Cleaned Data]
```

**Detailed Validation & Cleaning Steps:**

| Step | Field | Actions | Validation |
|------|-------|---------|------------|
| 1 | All | Drop completely empty rows | ‚úÖ Count logged |
| 2 | `judul` | Fill NA with empty string<br/>Strip whitespace<br/>Replace multiple spaces with single<br/>Remove newlines/tabs | ‚úÖ Normalized text |
| 3 | `deskripsi` | Fill NA with empty string<br/>Strip whitespace<br/>Replace multiple spaces<br/>Remove newlines/tabs | ‚úÖ Normalized text |
| 4 | `tanggal_artikel` | Fill NA with empty string<br/>Strip whitespace | ‚úÖ No nulls |
| 5 | `scraped_date` | Parse to datetime<br/>Fill errors with current datetime | ‚úÖ Datetime type |
| 6 | `url` | Fill NA with empty string<br/>Strip whitespace | ‚úÖ No nulls |
| 7 | `category` | Fill NA with 'Uncategorized'<br/>Strip whitespace<br/>Remove 'detik' prefix<br/>Replace empty with 'Uncategorized' | ‚úÖ Standardized |
| 8a | Duplicates (URL) | Remove duplicates on `url` | ‚úÖ URL duplicates logged |
| 8b | Duplicates (Title) | Remove duplicates on `judul` | ‚úÖ Title duplicates logged |
| 9 | Invalid Records | Remove rows where `judul` is empty | ‚úÖ Count logged |
| 10 | `article_id` | Reset sequential ID (1, 2, 3...) | ‚úÖ Unique IDs |

**Text Cleaning Regex:**
- `\s+` ‚Üí Single space (collapse multiple spaces)
- `[\r\n\t]` ‚Üí Space (remove line breaks, tabs)

**Data Quality Checks:**
- ‚úÖ **Missing Values:** All handled
- ‚úÖ **Text Normalization:** Whitespace, special chars cleaned
- ‚úÖ **Duplicates:** Removed by URL and title
- ‚úÖ **Data Integrity:** No records without title
- ‚úÖ **DateTime Parsing:** Scraped date in proper format

**Output Statistics:**
- Final shape
- Category distribution (value_counts)
- Columns list

---

## üì• PHASE 3: LOAD

### 3.1 Create Data Warehouse Schema

**Task:** `CreateWarehouseTables`

**Process Flow:**
```mermaid
graph TB
    A[Connect to Warehouse DB] --> B[Drop Existing Tables]
    B --> C[Create etl_log]
    C --> D[Create dim_date]
    D --> E[Create dim_products]
    E --> F[Create fact_sales]
    F --> G[Create nlp_training_data]
    G --> H[Verify Tables Created]
    H --> I[Create Flag File]
```

**Schema Design:**

| Table | Type | Purpose | Primary Key | Foreign Keys | Unique Constraints |
|-------|------|---------|-------------|--------------|-------------------|
| `etl_log` | Metadata | Track incremental loads | `log_id` (SERIAL) | - | - |
| `dim_date` | Dimension | Date dimension (2 years) | `date_id` (SERIAL) | - | `full_date` |
| `dim_products` | Dimension | Electronics catalog | `product_id` (SERIAL) | - | - |
| `fact_sales` | Fact | Amazon sales transactions | `sale_id` (SERIAL) | `date_id` ‚Üí dim_date | `(product_name, product_link)` |
| `nlp_training_data` | Training | NLP articles | `text_id` (SERIAL) | - | `url` |

**Indexes Created:**
- `dim_date`: full_date, (year, month)
- `dim_products`: brand, sub_category
- `fact_sales`: date_id, main_category, ratings
- `nlp_training_data`: category

**Validation Checks:**
- ‚úÖ All 5 tables created
- ‚úÖ Indexes created
- ‚úÖ Foreign keys configured
- ‚úÖ Unique constraints set (for UPSERT)

---

### 3.2 Load Date Dimension

**Task:** `LoadDimDate`

**Process Flow:**
```mermaid
graph LR
    A[Check Existing Count] --> B{Count > 0?}
    B -->|Yes| C[Skip Loading]
    B -->|No| D[Generate 2 Years of Dates]
    D --> E[Create DataFrame]
    E --> F[Calculate Attributes]
    F --> G[Bulk Insert]
    G --> H[Create Flag]
```

**Date Attributes Generated:**
- `full_date` (DATE, unique)
- `year`, `month`, `day`
- `quarter` (1-4)
- `day_of_week` (0-6)
- `day_name` (Monday, Tuesday...)
- `month_name` (January, February...)
- `is_weekend` (TRUE/FALSE)

**Validation Checks:**
- ‚úÖ 2 years = ~730 dates generated
- ‚úÖ No duplicates on full_date
- ‚úÖ All attributes calculated
- ‚úÖ Bulk insert successful

---

### 3.3 Load Products Dimension

**Task:** `LoadDimProducts`

**Input:** `cleaned/products_cleaned.csv`

**Process Flow:**
```mermaid
graph TB
    A[Check Existing Count] --> B[Load Cleaned Data]
    B --> C[Map to DW Columns]
    C --> D{First Load?}
    D -->|Yes| E[Bulk Append]
    D -->|No| F[UPSERT Mode]
    F --> G[Filter New Records]
    G --> H[Insert New Only]
    E --> I[Verify Count]
    H --> I
    I --> J[Create Flag]
```

**Column Mapping:**

| Source CSV | DW Column | Transformation |
|------------|-----------|----------------|
| `name` | `product_name` | Fill NA ‚Üí 'Unknown Product' |
| `brand` | `brand` | Fill NA ‚Üí 'Unknown Brand' |
| `manufacturer` | `manufacturer` | Fill NA ‚Üí 'Unknown' |
| `categories` | `main_category` | Fill NA ‚Üí 'Uncategorized' |
| `primaryCategories` | `sub_category` | Fill NA ‚Üí 'Uncategorized' |
| `prices.amountMin` | `prices_min` | Fill NA ‚Üí 0 |
| `prices.amountMax` | `prices_max` | Fill NA ‚Üí 0 |
| `prices.average` | `prices_average` | Fill NA ‚Üí 0 |
| `prices.currency` | `prices_currency` | Fill NA ‚Üí 'USD' |
| `prices.availability` | `availability` | Fill NA ‚Üí 'Unknown' |
| `prices.condition` | `condition` | Fill NA ‚Üí 'Unknown' |

**Validation Checks:**
- ‚úÖ Schema verified
- ‚úÖ Before/after count logged
- ‚úÖ UPSERT logic (skip existing, insert new)

---

### 3.4 Load Sales Fact Table

**Task:** `LoadFactSales`

**Input:** `cleaned/sales_cleaned.csv`

**Process Flow:**
```mermaid
graph TB
    A[Load Cleaned Sales] --> B[Get Date Dimension IDs]
    B --> C[Assign Random date_id]
    C --> D[Map to Fact Columns]
    D --> E[Cap Price Ranges]
    E --> F{First Load?}
    F -->|Yes| G[Bulk Insert All]
    F -->|No| H[UPSERT Mode]
    H --> I[CREATE TEMP TABLE]
    I --> J[INSERT Missing Records]
    J --> K[UPDATE Existing Records]
    K --> L[DROP TEMP TABLE]
    G --> M[Log to etl_log]
    L --> M
    M --> N[Create Flag]
```

**UPSERT Logic:**
```sql
-- Create temp table
CREATE TEMP TABLE temp_sales AS SELECT * FROM new_data;

-- Insert only new records (ON CONFLICT DO NOTHING)
INSERT INTO fact_sales (columns...)
SELECT * FROM temp_sales
ON CONFLICT (product_name, product_link) DO UPDATE SET
    discount_price = EXCLUDED.discount_price,
    actual_price = EXCLUDED.actual_price,
    ratings = EXCLUDED.ratings,
    updated_at = CURRENT_TIMESTAMP;

DROP TABLE temp_sales;
```

**Column Mapping & Validation:**

| Source | DW Column | Validation |
|--------|-----------|------------|
| `name` | `product_name` | Fill NA ‚Üí 'Unknown' |
| `main_category` | `main_category` | Fill NA ‚Üí 'Uncategorized' |
| `sub_category` | `sub_category` | Fill NA ‚Üí 'General' |
| Random | `date_id` | FK to dim_date |
| `discount_price` | `discount_price` | Numeric, cap to 9,999,999,999,999.99 |
| `actual_price` | `actual_price` | Numeric, cap to 9,999,999,999,999.99 |
| `discount_percentage` | `discount_percentage` | Numeric, cap to 999.99 |
| `ratings` | `ratings` | Numeric, cap to 5.00 |
| `no_of_ratings` | `no_of_ratings` | Integer |
| `image` | `image_url` | Text |
| `link` | `product_link` | Text |

**Validation Checks:**
- ‚úÖ Before/after count comparison
- ‚úÖ New records count logged
- ‚úÖ Price range validated (0 to max DECIMAL)
- ‚úÖ Ratings capped (0-5)
- ‚úÖ ETL log updated
- ‚úÖ UPSERT prevents duplicates

---

### 3.5 Load NLP Training Data

**Task:** `LoadNLPTrainingData`

**Input:** `cleaned/reviews_cleaned.csv`

**Process Flow:**
```mermaid
graph TB
    A[Load Cleaned Reviews] --> B[Parse scraped_date]
    B --> C[Filter Empty Titles]
    C --> D{First Load?}
    D -->|Yes| E[Bulk Insert All]
    D -->|No| F[UPSERT Mode]
    F --> G[CREATE TEMP TABLE]
    G --> H[INSERT Missing URLs]
    H --> I[UPDATE Existing URLs]
    I --> J[DROP TEMP TABLE]
    E --> K[Log to etl_log]
    J --> K
    K --> L[Create Flag]
```

**UPSERT Logic:**
```sql
-- Create temp table
CREATE TEMP TABLE temp_nlp AS SELECT * FROM new_data;

-- UPSERT on url (unique constraint)
INSERT INTO nlp_training_data (columns...)
SELECT * FROM temp_nlp
ON CONFLICT (url) DO UPDATE SET
    judul = EXCLUDED.judul,
    deskripsi = EXCLUDED.deskripsi,
    category = EXCLUDED.category,
    updated_at = CURRENT_TIMESTAMP;

DROP TABLE temp_nlp;
```

**Column Mapping:**

| Source | DW Column | Validation |
|--------|-----------|------------|
| `article_id` | `article_id` | Integer |
| `judul` | `judul` | Not empty |
| `deskripsi` | `deskripsi` | Text |
| `category` | `category` | Text |
| `url` | `url` | Unique constraint |
| `tanggal_artikel` | `tanggal_artikel` | Text |
| `scraped_date` | `scraped_date` | Datetime |

**Validation Checks:**
- ‚úÖ Empty titles filtered out
- ‚úÖ Before/after count logged
- ‚úÖ New records count logged
- ‚úÖ ETL log updated
- ‚úÖ UPSERT by URL (unique constraint)

---

## üîÑ Pipeline Execution & Dependencies

**Luigi Task Dependency Graph:**

```mermaid
graph TB
    subgraph Extract
        E1[ExtractSalesData]
        E2[ExtractProductsData]
        E3[ExtractDetikArticles]
    end
    
    subgraph Transform
        T1[TransformSalesData]
        T2[TransformProductsData]
        T3[TransformReviewsData]
    end
    
    subgraph Load
        L0[CreateWarehouseTables]
        L1[LoadDimDate]
        L2[LoadDimProducts]
        L3[LoadFactSales]
        L4[LoadNLPTrainingData]
    end
    
    E1 --> T1
    E2 --> T2
    E3 --> T3
    
    L0 --> L1
    L0 --> L2
    L0 --> L3
    L0 --> L4
    
    T2 --> L2
    T1 --> L3
    T3 --> L4
    
    L1 --> L3
    
    style E1 fill:#e1f5ff
    style E2 fill:#e1f5ff
    style E3 fill:#e1f5ff
    style T1 fill:#fff7e1
    style T2 fill:#fff7e1
    style T3 fill:#fff7e1
    style L0 fill:#e8f5e9
    style L1 fill:#e8f5e9
    style L2 fill:#e8f5e9
    style L3 fill:#e8f5e9
    style L4 fill:#e8f5e9
```

---

## ‚úÖ Complete Data Quality Checklist

### Extract Phase
- [ ] Database connections successful
- [ ] Source files exist and readable
- [ ] Web scraping retry mechanism working
- [ ] Session flags created for idempotency
- [ ] Raw data saved successfully

### Transform Phase  
- [ ] Missing values handled appropriately
- [ ] Data types enforced correctly
- [ ] Duplicates removed by business keys
- [ ] Range validation applied (ratings, prices)
- [ ] Text normalization completed
- [ ] Logical defaults applied
- [ ] Cross-field validation (min ‚â§ max)
- [ ] Cleaned data saved successfully

### Load Phase
- [ ] Warehouse schema created with constraints
- [ ] Indexes created for performance
- [ ] Dimension tables populated
- [ ] Fact tables populated
- [ ] UPSERT logic working correctly
- [ ] Foreign key integrity maintained
- [ ] ETL log updated
- [ ] Before/after counts validated

---

## üéØ Key Features

### 1. **Idempotency**
- Session flags prevent duplicate runs
- UPSERT prevents duplicate inserts
- `complete()` methods check both file AND flag

### 2. **Data Quality**
- **Missing Values:** All handled with business-appropriate defaults
- **Duplicates:** Removed at transform phase
- **Type Safety:** Enforced conversions with error handling
- **Range Validation:** Prices, ratings capped to valid ranges

### 3. **Incremental Loading**
- UPSERT on unique constraints
- Only new records inserted on subsequent runs
- ETL log tracks run history

### 4. **Error Handling**
- Retry logic for web scraping (3x with backoff)
- Timeout handling for unstable connections
- FileNotFoundError handling
- Database connection error handling

### 5. **Observability**
- Detailed logging at each step
- Before/after counts
- Data quality statistics reported
- Execution flags for tracking

---

## üìà Data Quality Metrics

| Metric | Location | Purpose |
|--------|----------|---------|
| Row count before/after | All transforms | Validate data loss |
| Duplicate count | Transform phase | Track cleaning effectiveness |
| Missing value handling | Transform phase | Ensure completeness |
| Category distribution | Reviews transform | Validate NLP data balance |
| Price range | Sales/Products | Detect outliers |
| Average rating | Sales transform | Quick quality check |
| New records count | Load phase | Track incremental growth |

---

## üöÄ Execution

```bash
# Full pipeline
python main.py

# Individual phases
python Task/extract.py  # Extract only
python Task/transform.py  # Transform only
python Task/load.py  # Load only
```

---

**Created:** 2026-02-17  
**Pipeline Version:** 1.0  
**Author:** ETL Team
